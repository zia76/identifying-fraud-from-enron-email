Udacity Data Analyst Nanodegree
Project 4: Identifying Enron Corporate Fraud
Answers to Project Questions
Kaleem Khwaja
May 4, 2015


1.
The goal of this project was to identify persons of interest (POIs) using a dataset of combined Enron financial and email data. POIs are people who were either found guilty of wrongdoing in the historic Enron scandal of a decade ago, or who were indicted or made some kind of deal with the government, whether a cash settlement, a plea deal, or immunity in return for their testimony. The dataset includes financial information covering 14 features, all in dollars, such as salary and bonuses, 6 numeric features summarizing the email data, and most importantly, categorical 'POI' labels, telling whether or not a person is a known POI. The data covers 146 data points, i.e. people, 18 of whom are labeled as known POIs.

Supervised machine learning algorithms are ideal tools for building a classifier that can recognize POIs. A good tool has the potential to identify POI-like patterns in the data and use these patterns to uncover previously unknown POIs.

Outliers: The only real outlier I found in the data was the non-person 'TOTAL': it was the column totals for each feature and didn't belong in the dataset, so I removed it by hand. There was one other non-person in the list of names, "THE TRAVEL AGENCY IN THE PARK". I left it in because it did not present as an outlier, and could have been an interesting result if it was categorized as a POI by my model. I used histograms and scatterplots of different features to identify other outliers, but didn't find any that I thought were the results of errors in the data.

2. and 3.
I started out feeding all features into six different algorithms with default parameters (Naive Bayes, Decision Tree, Random Forest, AdaBoost, K-Nearest Neighbors, and SVM). I then removed all financial info to see how the email features would hold up on their own, and vice-versa. Performance varied based on algorithm, but generally financial data alone was more useful than email data alone, and the combination was superior to either alone.

New Features: I then engineered two new features: 'email_from_poi_rate' and 'email_to_poi_rate'. The former was simply the number of emails from a POI divided by total incoming emails. I figured that the proportion of emails coming from a POI was more significant than the absolute number of them, given that some people simply use email more than others. These features dramatically improved performance for decision trees and random forests, and had little to no effect on the other algorithms. This makes perfect sense and could have been predicted a priori: tree-based methods can only create decision boundaries orthogonal to existing features, so adding these derivative features gave the tree methods another axis on which to split, and one which was apparently more informative than the orginal email features. The clustering algorithms, on the other hand, can create decision boundaries in any hyperplane within the data, so a derivative feature made by dividing one feature by another adds relatively little extra information or capacity to the system.

In order to create these new features I needed to write a new function to replace NaNs in the email data. Of the 145 rows in the cleaned dataset, 59 had only NaN values for all email-related features. I could not rely on the built-in featureFormat function to replace NaN with 0, because I could not allow zeros in the denominator of my new feature calculations. For this reason, I replaced all NaNs in email features with their column means. I later tried (for pedagogical value) replacing column means with a uniform value of 1, and observed no resulting shift in performance.

Data processing: I used the MinMaxScaler module to scale the features and expected it to increase the performance of SVM and KNN algorithms. It had no effect at all. I looked at the input and output data to be sure scaling was working properly, and it was.

Model/Feature Selection: I used an iterative process of hand-picking combinations of features and algorithm before settling in on what seemed to be at least a local minimum in performance. The best algorithm for this data by a solid margin was K-Nearest Neighbors (2nd place was AdaBoost and KNN outperformed AdaBoost by 50% in F1 score). The final 6 built-in features I used were: 'poi', 'shared_receipt_with_poi', 'bonus', 'exercised_stock_options', 'other', and 'deferral_payments'. I also included my two new features, which slightly increased performance versus the stock 6 alone (added 0.007 to F1 score). Beyond these 8 features, no additional feature inproved classification performance. I also created a pipeline that fed the results of tuned PCA into KNN, but could not improve upon the performance of my hand-picked features.

4.
Parameter Tuning: Parameter tuning is critical to the KNN algorithm: results will vary greatly depending principally on the number of neighbors used to vote on the cluster class of each data point. The weight given to each neighbor can also be adjusted: 'uniform' weights each of the n neighbors equally, whereas 'distance' weights neighbors as a function of their proximity to the point being classified (i.e. a closer point has a stonger 'vote'). I used the GridSearch module to tune parameters once I settled in on KNN (I used GridSearch on other algorithms as well, just to be sure they wouldn't outperform KNN once tuned. I explored tuning C, gamma and kernel for the SVM, but no parameter settings were able to lift the SVM's performance out of last place). I adjusted two parameters within KNN: 'n_neighbors' and 'method', and explored the entire space from 3 to 10 neighbors and weight types 'uniform' and 'distance'. The best-performing parameter pairing was n_neighbors = 5 and weights = 'distance'.

Performance Metrics: The tester.py script returned a variety of measures of model performance, including accuracy, precision, recall, and F1 and F2 score. Because only 12.5% of the data points were POIs, one could achieve an accuracy of 87.5% by classifying all points as non-POI. Therefore, accuracy was not the best indicator of a good model. I relied on a combination of precision, recall and F1 score. F1 score is essentially a weighted average of precision and recall, so I considered a high F1 score the most important measure of success.

Depending on your goals, one might prefer a model with higher precision or recall versus F1. For example, if your goal was a model which could identify all POIs, but didn't care about false positives, high recall would be the metric to aim for. If, on the other hand, you wanted to be very careful not to implicate anyone falsely as a POI, high precision would be more important. Since the project did not specify an objective in this regard, I relied primarily on the more general F1 score.

5.
Validation: Validation is testing the performance of your algorithm on a 'test' set of data points not used in the training process. Results on a test set give a generalized performance metric, and, if much lower than performance on the training set, are an indication of overfitting to the training set.

Validation was built into the tester.py script using a Stratified Shuffle Split method. This method holds out a random sample of 10% of the data to be used as a test set, and because the dataset is so small, this process is iterated 1000 times and the results pooled.

Ideally, I would have been able to train my model on data completely separate from that used for testing, but since the tester.py test sets are randomly sampled from the entire dataset, there was no way for me to train on data that would not later be used for testing. For this reason it would not have been statistically meaningful to use a separate cross-validation step upstream of the train/test process in the tester script, so I used the tester.py test results for model selection and parameter tuning. Generally, this is a giant no-no, but it was effectively unavoidable given the way the test script was set up (and ultimately, given the small size of the dataset).

6.
Results: My final model has an F1 score of 0.5483, with precision 0.736 and recall 0.437. In other words, of the 18 labeled POIs, it correctly labels those POIs as such 43.7% of the time. Of all persons it labels as a POI, 73.6% of them are in fact a POI.

Stepping away from the tehnical aspects, my model identifies 18 people as a POI at least once in the many iterations of testing. Of these 18, 8 are known POIs and 10 are not formerly known as POIs. All 8 known POIs are in the top 9 people most commonly labeled as POI. Of the 10 non-POIs most frequently labeled as POI by my classifier, my analysis suggests the following 3 former Enron employees as the most likely co-conspirators: Gary J Hickerson (a former CIA scientist and Enron hedge fund trader who received a $600,000 'retention' payment), Phillip K Allen (an energy trader who received a $4.4M bonus), and Lou L Pai (CEO of two Enron divisions who left Enron with over $270M, divorced his wife, and married an exotic dancer).